{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DIR = '/home/shreyas/Documents/git/wheelai/'\n",
    "#DATA_DIR = '/media/shreyas/OS/ML_DATA/wheelai/kerakart/'\n",
    "DATA_DIR = '/media/shreyas/OS/ML_DATA/wheelai/kerakart/sample/'\n",
    "traindata_path = DATA_DIR + 'train/'\n",
    "validdata_path = '/media/shreyas/OS/ML_DATA/wheelai/kerakart/valid/'\n",
    "results_path = '/media/shreyas/OS/ML_DATA/wheelai/kerakart/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, load_model,  Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import VGG16\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(path, class_mode='categorical', gen=image.ImageDataGenerator(rescale=1./255), \\\n",
    "                shuffle=True, target_size=(160,80), batch_size=1):\n",
    "    return gen.flow_from_directory(path, class_mode=class_mode, batch_size=batch_size, \\\n",
    "                                   target_size=target_size, shuffle=shuffle)\n",
    "\n",
    "def get_steps(batches, batch_size):\n",
    "    steps = int(batches.samples/batch_size)\n",
    "    return (steps if batches.samples%batch_size==0 else (steps+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1366 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_b = get_batches(traindata_path, batch_size=batch_size)\n",
    "valid_b = get_batches(validdata_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_steps = get_steps(train_b, batch_size)\n",
    "valid_steps = get_steps(valid_b, batch_size)\n",
    "num_class = train_b.num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_b.classes\n",
    "valid_labels = valid_b.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(train_labels)\n",
    "y_valid = keras.utils.to_categorical(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1366,) (300,) (1366, 3) (300, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape, valid_labels.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1366 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_b = get_batches(traindata_path, class_mode=None, shuffle=False, batch_size=batch_size)\n",
    "val_b = get_batches(validdata_path, class_mode=None, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1366 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_augdata = image.ImageDataGenerator(width_shift_range=.2, height_shift_range=.2, \n",
    "                                      shear_range=0.2, zoom_range=0.2)\n",
    "augtrain_b = get_batches(traindata_path, gen=gen_augdata, class_mode='categorical',\\\n",
    "                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_conv():\n",
    "    model = VGG16(include_top=False, weights='imagenet', input_shape=(160,80,3))\n",
    "    #remove the last convolutional block\n",
    "    [model.layers.pop() for _ in range(4)]\n",
    "    return model\n",
    "\n",
    "\n",
    "def top_layer(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(MaxPooling2D(2,2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='elu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_model(input_shape):\n",
    "    model = Sequential()\n",
    "    #model.add(BatchNormalization(axis=1, input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Flatten())    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ft(trn_b, train_steps, val_b, valid_steps):\n",
    "    model = vgg_conv()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    trn_ft = model.predict_generator(trn_b, train_steps)\n",
    "    val_ft = model.predict_generator(val_b, valid_steps)\n",
    "    print(trn_ft.shape, val_ft.shape)\n",
    "\n",
    "    np.save(results_path + 'trn_ft.npy', trn_ft)\n",
    "    np.save(results_path + 'val_ft.npy', val_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_ft(trn_b, train_steps, val_b, valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(results_path + 'trn_ft.npy')\n",
    "X_valid = np.load(results_path + 'val_ft.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = top_layer(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=2e-5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train, epochs=13, batch_size=1, verbose=1, \\\n",
    "#          validation_data=(X_valid, y_valid))\n",
    "#model.fit_generator(train_b, steps_per_epoch=trn_steps, epochs=10,\\\n",
    "#                   validation_data=valid_b, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath1 = results_path+'vgg_top.h5'\n",
    "checkpoint1 = ModelCheckpoint(filepath1, monitor='val_loss', verbose=1, save_best_only=True,\\\n",
    "                             mode='min', period=1)\n",
    "callbacks1=[checkpoint1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=21, batch_size=1, verbose=1, \\\n",
    "          callbacks=callbacks1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = vgg_conv()\n",
    "for layer in base_model.layers[:11]: layer.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#top_model = top_layer(base_model.output_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = load_model(results_path+'vgg_top.h5')\n",
    "#top_model.load_weights(results_path+'bottleneck_topmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = top_model(base_model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdwdwdwdwdwdwdwdwdwdwdwdwdwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath2 = results_path+'ft01.h5'\n",
    "checkpoint2 = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1, save_best_only=True,\\\n",
    "                             save_weights_only=True, mode='min', period=1)\n",
    "callbacks2=[checkpoint2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9640Epoch 00000: val_loss improved from inf to 0.32765, saving model to /media/shreyas/OS/ML_DATA/wheelai/kerakart/results/ft01.h5\n",
      "86/86 [==============================] - 10s - loss: 0.0892 - acc: 0.9644 - val_loss: 0.3277 - val_acc: 0.8933\n",
      "Epoch 2/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9728Epoch 00001: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0723 - acc: 0.9731 - val_loss: 0.3548 - val_acc: 0.8867\n",
      "Epoch 3/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9779Epoch 00002: val_loss improved from 0.32765 to 0.30846, saving model to /media/shreyas/OS/ML_DATA/wheelai/kerakart/results/ft01.h5\n",
      "86/86 [==============================] - 8s - loss: 0.0606 - acc: 0.9782 - val_loss: 0.3085 - val_acc: 0.9133\n",
      "Epoch 4/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9904Epoch 00003: val_loss improved from 0.30846 to 0.28456, saving model to /media/shreyas/OS/ML_DATA/wheelai/kerakart/results/ft01.h5\n",
      "86/86 [==============================] - 8s - loss: 0.0416 - acc: 0.9906 - val_loss: 0.2846 - val_acc: 0.9267\n",
      "Epoch 5/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9904Epoch 00004: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0433 - acc: 0.9906 - val_loss: 0.3482 - val_acc: 0.8900\n",
      "Epoch 6/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9890Epoch 00005: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0383 - acc: 0.9891 - val_loss: 0.3738 - val_acc: 0.8933\n",
      "Epoch 7/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9926Epoch 00006: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0356 - acc: 0.9927 - val_loss: 0.3047 - val_acc: 0.9033\n",
      "Epoch 8/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9949Epoch 00007: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0316 - acc: 0.9949 - val_loss: 0.2999 - val_acc: 0.9067\n",
      "Epoch 9/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9949Epoch 00008: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0306 - acc: 0.9949 - val_loss: 0.4501 - val_acc: 0.8933\n",
      "Epoch 10/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9956Epoch 00009: val_loss improved from 0.28456 to 0.27622, saving model to /media/shreyas/OS/ML_DATA/wheelai/kerakart/results/ft01.h5\n",
      "86/86 [==============================] - 8s - loss: 0.0261 - acc: 0.9956 - val_loss: 0.2762 - val_acc: 0.9300\n",
      "Epoch 11/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9971Epoch 00010: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0229 - acc: 0.9971 - val_loss: 0.3176 - val_acc: 0.9133\n",
      "Epoch 12/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9978Epoch 00011: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0235 - acc: 0.9978 - val_loss: 0.3935 - val_acc: 0.9167\n",
      "Epoch 13/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9993Epoch 00012: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0200 - acc: 0.9993 - val_loss: 0.2986 - val_acc: 0.9267\n",
      "Epoch 14/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9985Epoch 00013: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0194 - acc: 0.9985 - val_loss: 0.3592 - val_acc: 0.9000\n",
      "Epoch 15/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9978Epoch 00014: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0176 - acc: 0.9978 - val_loss: 0.4309 - val_acc: 0.9033\n",
      "Epoch 16/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9978Epoch 00015: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0194 - acc: 0.9978 - val_loss: 0.3597 - val_acc: 0.9067\n",
      "Epoch 17/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9993Epoch 00016: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0158 - acc: 0.9993 - val_loss: 0.3809 - val_acc: 0.9067\n",
      "Epoch 18/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9978Epoch 00017: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0157 - acc: 0.9978 - val_loss: 0.3931 - val_acc: 0.9200\n",
      "Epoch 19/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9978Epoch 00018: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0187 - acc: 0.9978 - val_loss: 0.3980 - val_acc: 0.9033\n",
      "Epoch 20/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9978Epoch 00019: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0153 - acc: 0.9978 - val_loss: 0.3136 - val_acc: 0.9167\n",
      "Epoch 21/21\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9985Epoch 00020: val_loss did not improve\n",
      "86/86 [==============================] - 8s - loss: 0.0163 - acc: 0.9985 - val_loss: 0.4099 - val_acc: 0.9033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f312f2dfbe0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=21, callbacks=callbacks2,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (preds[:5])\n",
    "img = batches.filenames\n",
    "print (img[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(DATA_DIR+'test/'+img[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(results_path + 'test_predictions.dat', preds)\n",
    "save_array(results_path + 'imagefiles.dat', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Predictions\n",
    "Lets plot -\n",
    "1. A few correct labels at random\n",
    "2. A few incorrect labels at random\n",
    "3. Most confident correct predictions of each class\n",
    "4. Most confident incorrect predictions of each class\n",
    "5. Most uncertain labels (probabilites close to 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches, probs = vgg.test(validdata_path, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = val_batches.filenames\n",
    "expected_labels = val_batches.classes\n",
    "\n",
    "our_predictions = probs[:,0]\n",
    "other_predictions = np.round(probs[:,1])\n",
    "our_labels = np.round(1-our_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def plots_idx(idx, titles=None):\n",
    "    plots([image.load_img(validdata_path + img[i]) for i in idx], titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_view = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = np.where(our_labels==expected_labels)[0]\n",
    "print (\"Found %d correct labels\" % len(correct))\n",
    "idx = permutation(correct)[:n_view]\n",
    "plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = np.where(our_labels!=expected_labels)[0]\n",
    "print (\"Found %d incorrect labels\" % len(incorrect))\n",
    "idx = permutation(incorrect)[:n_view]\n",
    "plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3a. The images we most confident were cats, and are actually cats\n",
    "correct_cats = np.where((our_labels==0) & (our_labels==expected_labels))[0]\n",
    "print (\"Found %d confident correct cats labels\" % len(correct_cats))\n",
    "most_correct_cats = np.argsort(our_predictions[correct_cats])[::-1][:n_view]\n",
    "plots_idx(correct_cats[most_correct_cats], our_predictions[correct_cats][most_correct_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3b. The images we most confident were dogs, and are actually dogs\n",
    "correct_dogs = np.where((our_labels==1) & (our_labels==expected_labels))[0]\n",
    "print (\"Found %d confident correct dogs labels\" % len(correct_dogs))\n",
    "most_correct_dogs = np.argsort(our_predictions[correct_dogs])[:n_view]\n",
    "plots_idx(correct_dogs[most_correct_dogs], our_predictions[correct_dogs][most_correct_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a. The images we were most confident were cats, but are actually dogs\n",
    "incorrect_cats = np.where((our_labels==0) & (our_labels!=expected_labels))[0]\n",
    "print (\"Found %d incorrect cats\" % len(incorrect_cats))\n",
    "if len(incorrect_cats):\n",
    "    most_incorrect_cats = np.argsort(our_predictions[incorrect_cats])[::-1][:n_view]\n",
    "    plots_idx(incorrect_cats[most_incorrect_cats], our_predictions[incorrect_cats][most_incorrect_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#4b. The images we were most confident were dogs, but are actually cats\n",
    "incorrect_dogs = np.where((our_labels==1) & (our_labels!=expected_labels))[0]\n",
    "print (\"Found %d incorrect dogs\" % len(incorrect_dogs))\n",
    "if len(incorrect_dogs):\n",
    "    most_incorrect_dogs = np.argsort(our_predictions[incorrect_dogs])[:n_view]\n",
    "    plots_idx(incorrect_dogs[most_incorrect_dogs], our_predictions[incorrect_dogs][most_incorrect_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5. The most uncertain labels (ie those with probability closest to 0.5).\n",
    "most_uncertain = np.argsort(np.abs(our_predictions-0.5))\n",
    "plots_idx(most_uncertain[:n_view], our_predictions[most_uncertain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels, our_labels)\n",
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_array(results_path + 'test_predictions.dat')\n",
    "filenames = load_array(results_path + 'imagefiles.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isdog = preds[:,1]\n",
    "print (\"Raw Predictions: \"+ str(isdog[:5]))\n",
    "print (\"Mid Predictions: \"+str(isdog[(isdog<.6)&(isdog>0.4)]))\n",
    "print (\"Edge Predictions: \"+str(isdog[(isdog<0.02)&(isdog>.98)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(isdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract imageIds from the filenames in our test/unknown directory \n",
    "filenames = batches.filenames\n",
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_DIR\n",
    "submission_file_name = 'submission1.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
