{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#d = '/media/shreyas/DATA/ML_DATA/wheelai/gtav/train/'\n",
    "d = '/media/shreyas/DATA/ML_DATA/wheelai/gtav/sample/'\n",
    "data_path = d + 'dataset.pz'\n",
    "val_data = '/media/shreyas/DATA/ML_DATA/wheelai/gtav/valid/dataset.pz'\n",
    "\n",
    "x_trn_path = d + 'x_trn.bc'\n",
    "y_trn_path = d + 'y_trn.bc'\n",
    "\n",
    "s_crp_path = d + 's_crp.bc'\n",
    "x_crp_path = '/home/shreyas/Downloads/data/x_crp.bc'\n",
    "v_crp_path = '/home/shreyas/Downloads/data/v_crp.bc'\n",
    "\n",
    "x_val_path = '/media/shreyas/DATA/ML_DATA/wheelai/gtav/valid/x_val.bc'\n",
    "y_val_path = '/media/shreyas/DATA/ML_DATA/wheelai/gtav/valid/y_val.bc'\n",
    "results_path = '/media/shreyas/DATA/ML_DATA/wheelai/gtav/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, load_model,  Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import Input, Lambda, Cropping2D, Activation, ELU\n",
    "from keras.layers.merge import add, concatenate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches with [Keras ImageDataGenerator](https://keras.io/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "data_dim = (160,320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batches(path, class_mode='categorical', gen=image.ImageDataGenerator(), \\\n",
    "                shuffle=True, target_size=data_dim, batch_size=1):\n",
    "    '''\n",
    "    Args\n",
    "    path: path to data directory\n",
    "    calss_mode: 'categorical', 'binary', 'sparse'\n",
    "    gen: keras image generator\n",
    "    shuffle: if to shuffle data or not\n",
    "    target_size: out dimensions of the image\n",
    "    Yields\n",
    "    batch of given dimension\n",
    "    '''\n",
    "    return gen.flow_from_directory(path, class_mode=class_mode, batch_size=batch_size, \\\n",
    "                                   target_size=target_size, shuffle=shuffle)\n",
    "\n",
    "def get_steps(batches, batch_size):\n",
    "    '''Return number of times the batches to train on for keras fit_generator'''\n",
    "    steps = int(batches.samples/batch_size)\n",
    "    return (steps if batches.samples%batch_size==0 else (steps+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12954 images belonging to 3 classes.\n",
      "Found 3375 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_b = get_batches(traindata_path, batch_size=batch_size)\n",
    "valid_b = get_batches(validdata_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_steps = get_steps(train_b, batch_size)\n",
    "valid_steps = get_steps(valid_b, batch_size)\n",
    "num_class = train_b.num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train_b.classes\n",
    "valid_labels = valid_b.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the labels\n",
    "y_train = keras.utils.to_categorical(train_labels)\n",
    "y_valid = keras.utils.to_categorical(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12954,) (3375,) (12954, 3) (3375, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape, valid_labels.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12954 images belonging to 3 classes.\n",
      "Found 3375 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# generate batches to calculating the conv features\n",
    "# NOTE: CLASS_MODE = NONE because you we are running batches for prediction and not training\n",
    "trn_b = get_batches(traindata_path, class_mode=None, shuffle=False, batch_size=batch_size)\n",
    "val_b = get_batches(validdata_path, class_mode=None, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More about Data Augmentation with Keras [here](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) and [here.](http://machinelearningmastery.com/image-augmentation-deep-learning-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12954 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Augment data\n",
    "gen_augdata = image.ImageDataGenerator(width_shift_range=.2, height_shift_range=.2, \n",
    "                                      shear_range=0.2, zoom_range=0.2)\n",
    "augtrain_b = get_batches(traindata_path, gen=gen_augdata, class_mode='categorical',\\\n",
    "                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More of Nvidia's [End to End Learning Model](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nvidia_model():\n",
    "    '''Implements Nvidia's end to end learning model'''\n",
    "    # Croping the image to retain only road part of image\n",
    "    x = Input(shape=(160, 320, 3))\n",
    "    crop_im = Cropping2D(cropping=((70,0), (0,0)))(x)\n",
    "    # layer to learn color space\n",
    "    color_s = Conv2D(3, (1,1), padding='same')(crop_im)\n",
    "    # normalise to mean 0\n",
    "    norm = Lambda(lambda x: (x / 255.0) - 0.5)(color_s)\n",
    "    \n",
    "    conv_b1 = Conv2D(24, (3,3), padding='same', activation='relu')(norm)\n",
    "    batch_1 = BatchNormalization()(conv_b1)\n",
    "    max_pl1 = MaxPooling2D()(batch_1)\n",
    "    \n",
    "    conv_b2 = Conv2D(36, (3,3), padding='same', activation='relu')(max_pl1)\n",
    "    batch_2 = BatchNormalization()(conv_b2)\n",
    "    max_pl2 = MaxPooling2D()(batch_2)\n",
    "    \n",
    "    conv_b3 = Conv2D(48, (3,3), padding='same', activation='relu')(max_pl2)\n",
    "    batch_3 = BatchNormalization()(conv_b3)\n",
    "    max_pl3 = MaxPooling2D()(batch_3)\n",
    "    \n",
    "    conv_b4 = Conv2D(64, (3,3), padding='same', activation='relu')(max_pl3)\n",
    "    batch_4 = BatchNormalization()(conv_b4)\n",
    "    max_pl4 = MaxPooling2D()(batch_4)\n",
    "    dropout = Dropout(0.1)(max_pl4)\n",
    "    \n",
    "    conv_b5 = Conv2D(64, (3,3), padding='same', activation='relu')(dropout)\n",
    "    batch_5 = BatchNormalization()(conv_b5)\n",
    "    max_pl5 = MaxPooling2D((1,4))(batch_5)\n",
    "    dropout = Dropout(0.1)(max_pl5)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    fc1 = Dense(100, activation='relu')(flatten)\n",
    "    batch_fc1 = BatchNormalization()(fc1)\n",
    "    drop_fc1 = Dropout(0.2)(batch_fc1)\n",
    "    \n",
    "    fc2 = Dense(50, activation='relu')(drop_fc1)\n",
    "    batch_fc2 = BatchNormalization()(fc2)\n",
    "    drop_fc2 = Dropout(0.3)(batch_fc2)\n",
    "    \n",
    "    fc3 = Dense(10, activation='relu')(drop_fc2)\n",
    "    batch_fc3 = BatchNormalization()(fc3)\n",
    "    drop_fc3 = Dropout(0.5)(batch_fc3)\n",
    "    \n",
    "    output = Dense(3, activation='softmax')(drop_fc3)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nvidia_conv():\n",
    "    '''Implement a model similar to Nvidia end to end but with only conv layers'''\n",
    "    model = Sequential()\n",
    "    model.add(Cropping2D(cropping=((70,0), (0,0)), input_shape=(160, 320, 3)))\n",
    "    model.add(Lambda(lambda x: (x / 255.0) - 0.5))\n",
    "    model.add(Conv2D(3, (1,1), padding='same')) \n",
    "    model.add(Conv2D(24, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(36, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())   \n",
    "    model.add(Conv2D(48, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((1,4)))\n",
    "    model.add(Conv2D(3, (3,3), padding='same'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    '''Implement a model similar to VGG'''\n",
    "    model = Sequential()\n",
    "    model.add(Cropping2D(cropping=((70,0), (0,0)), input_shape=(160, 320, 3)))\n",
    "    model.add(Lambda(lambda x: (x / 255.0) - 0.5))\n",
    "    model.add(Conv2D(3, (1,1), padding='same')) \n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def amazon_model():\n",
    "    '''Implement a starting model for Amazon forest classification competition on kaggle'''\n",
    "    x = Input(shape = (160, 320, 3))\n",
    "    crop = Cropping2D(cropping=((70,0), (0,0)), input_shape=(160, 320, 3))(x)\n",
    "    norm = Lambda(lambda x: (x / 255.0) - 0.5)(crop)\n",
    "    \n",
    "    prep = Conv2D(10, (1,1), padding='same', activation='elu')(norm)\n",
    "    prep = Conv2D(10, (1,1), padding='same', activation='elu')(prep)\n",
    "    prep = Conv2D(3, (1,1), padding='same', activation='elu')(prep)\n",
    "    \n",
    "    conv1 = Conv2D(16, (3,3), padding='same', activation='relu')(prep)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (1,1), padding='same', activation='relu')(norm1)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (1,1), padding='same', activation='relu')(norm1)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3,3), padding='same', activation='relu')(norm1)\n",
    "    norm1 = BatchNormalization()(conv1)\n",
    "    max_1 = MaxPooling2D()(norm1)\n",
    "    \n",
    "    conv2 = Conv2D(32, (3,3), padding='same', activation='relu')(max_1)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(48, (1,1), padding='same', activation='relu')(norm2)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(48, (1,1), padding='same', activation='relu')(norm2)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(48, (3,3), padding='same', activation='relu')(norm2)\n",
    "    norm2 = BatchNormalization()(conv2)\n",
    "    max_2 = MaxPooling2D()(norm2)\n",
    "    gavg1 = GlobalAveragePooling2D()(max_2)\n",
    "    \n",
    "    conv3 = Conv2D(48, (3,3), padding='same', activation='relu')(max_2)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(64, (1,1), padding='same', activation='relu')(norm3)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(64, (1,1), padding='same', activation='relu')(norm3)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(64, (3,3), padding='same', activation='relu')(norm3)\n",
    "    norm3 = BatchNormalization()(conv3)\n",
    "    max_3 = MaxPooling2D()(norm3)\n",
    "    gavg2 = GlobalAveragePooling2D()(max_3)\n",
    "    \n",
    "    conv4 = Conv2D(64, (3,3), padding='same', activation='relu')(max_3)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(128, (1,1), padding='same', activation='relu')(norm4)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(128, (1,1), padding='same', activation='relu')(norm4)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(128, (3,3), padding='same', activation='relu')(norm4)\n",
    "    norm4 = BatchNormalization()(conv4)\n",
    "    max_4 = MaxPooling2D()(norm4)\n",
    "    gavg3 = GlobalAveragePooling2D()(max_4)\n",
    "    \n",
    "    merged = concatenate([gavg1, gavg2, gavg3])\n",
    "    fc_1 = Dense(240, activation='relu')(merged)\n",
    "    norm = BatchNormalization()(fc_1)\n",
    "    dropout = Dropout(0.5)(norm)\n",
    "    fc_2 = Dense(120, activation='relu')(dropout)\n",
    "    norm = BatchNormalization()(fc_2)\n",
    "    dropout = Dropout(0.5)(norm)\n",
    "    outout = Dense(3, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=outout)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Nvidia Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nvidia_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath1 = results_path+'nvidia.h5'\n",
    "#checkpoint1 = ModelCheckpoint(filepath1, monitor='val_loss', verbose=1, save_best_only=False,\\\n",
    "#                             save_weights_only=True, mode='min', period=1)\n",
    "#callbacks1=[checkpoint1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_path = results_path+'nvidia.json'\n",
    "model_json = model.to_json()\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia(1e-4).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia(1e-5).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=1, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia(1e-3).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath2 = results_path+'nvidia_aug.h5'\n",
    "#checkpoint2 = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1, save_best_only=False,\\\n",
    "#                             save_weights_only=True, mode='min', period=1)\n",
    "#callbacks2=[checkpoint2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_aug(1e-4).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_aug(1e-3).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_aug(1e-5).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Nvidia All CONV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_1 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 90, 320, 3)        12        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 90, 320, 24)       672       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 90, 320, 24)       96        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 45, 160, 24)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 45, 160, 36)       7812      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 45, 160, 36)       144       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 80, 36)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 22, 80, 48)        15600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 22, 80, 48)        192       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 11, 40, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 40, 64)        27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 11, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 5, 20, 128)        73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 5, 20, 128)        512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 5, 3)           3459      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 130,323\n",
      "Trainable params: 129,723\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = nvidia_conv()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_path = results_path+'nvidia_conv.json'\n",
    "model_json = model.to_json()\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(results_path+'nvidia_conv_aug(1e-5).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "810/810 [==============================] - 232s - loss: 0.8711 - val_loss: 1.0283\n",
      "Epoch 2/2\n",
      "810/810 [==============================] - 226s - loss: 0.8648 - val_loss: 0.9870\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_conv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "810/810 [==============================] - 228s - loss: 0.8442 - val_loss: 1.0649\n",
      "Epoch 2/3\n",
      "810/810 [==============================] - 226s - loss: 0.8272 - val_loss: 1.0568\n",
      "Epoch 3/3\n",
      "810/810 [==============================] - 227s - loss: 0.8142 - val_loss: 1.0242\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_conv2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "810/810 [==============================] - 229s - loss: 0.8084 - val_loss: 0.9620\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=1, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_conv3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath2 = results_path+'nvidia_aug.h5'\n",
    "#checkpoint2 = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1, save_best_only=False,\\\n",
    "#                             save_weights_only=True, mode='min', period=1)\n",
    "#callbacks2=[checkpoint2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "810/810 [==============================] - 229s - loss: 0.7897 - val_loss: 0.9965\n",
      "Epoch 2/3\n",
      "810/810 [==============================] - 226s - loss: 0.7809 - val_loss: 1.0148\n",
      "Epoch 3/3\n",
      "810/810 [==============================] - 227s - loss: 0.7631 - val_loss: 1.0017\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_conv4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "810/810 [==============================] - 229s - loss: 0.7510 - val_loss: 0.9282\n",
      "Epoch 2/2\n",
      "810/810 [==============================] - 227s - loss: 0.7511 - val_loss: 0.9684\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_conv5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "810/810 [==============================] - 174s - loss: 0.6059 - val_loss: 0.8650\n",
      "Epoch 2/3\n",
      "810/810 [==============================] - 172s - loss: 0.5473 - val_loss: 0.8547\n",
      "Epoch 3/3\n",
      "810/810 [==============================] - 172s - loss: 0.5084 - val_loss: 0.8319\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'nvidia_conv6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Amazon Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = amazon_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'amazon(1e-4).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'amazon(1e-5).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=1, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'amazon(1e-3).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath2 = results_path+'nvidia_aug.h5'\n",
    "#checkpoint2 = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1, save_best_only=False,\\\n",
    "#                             save_weights_only=True, mode='min', period=1)\n",
    "#callbacks2=[checkpoint2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'amazon_aug(1e-4).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'amazon_aug(1e-3).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'amazon_aug(1e-5).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_2 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 90, 320, 3)        12        \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 90, 320, 32)       896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 90, 320, 32)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 45, 160, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 45, 160, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 45, 160, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 45, 160, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 22, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 22, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 22, 80, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 11, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 56320)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               14418176  \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 14,580,943\n",
      "Trainable params: 14,579,471\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = base_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_path = results_path+'base_model.json'\n",
    "model_json = model.to_json()\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(results_path+'base_aug(1e-5).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "810/810 [==============================] - 230s - loss: 1.0631 - val_loss: 1.0849\n",
      "Epoch 2/2\n",
      "810/810 [==============================] - 227s - loss: 1.0254 - val_loss: 0.9880\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'base1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "140/810 [====>.........................] - ETA: 154s - loss: 1.0191"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'base2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=1, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'base3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath2 = results_path+'nvidia_aug.h5'\n",
    "#checkpoint2 = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1, save_best_only=False,\\\n",
    "#                             save_weights_only=True, mode='min', period=1)\n",
    "#callbacks2=[checkpoint2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=3, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'base4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'base5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=2, verbose=1,\\\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "\n",
    "model.save_weights(results_path+'base6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
