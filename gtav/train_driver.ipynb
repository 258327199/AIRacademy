{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CODE_DIR = '/home/shreyas/Documents/git/wheelai/'\n",
    "traindata_path = '/media/shreyas/DATA/ML_DATA/wheelai/gtaV/sample_train/'\n",
    "validdata_path = '/media/shreyas/DATA/ML_DATA/wheelai/gtaV/valid/'\n",
    "results_path = '/media/shreyas/DATA/ML_DATA/wheelai/gtaV/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, load_model,  Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import Lambda, Cropping2D, Activation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create helper fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_dim = (160, 320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reading on [keras Imagegenerator](https://keras.io/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batches(path, class_mode='categorical', gen=image.ImageDataGenerator(), \\\n",
    "                shuffle=True, target_size=data_dim, batch_size=1):\n",
    "    '''\n",
    "    Args\n",
    "    path: path to data directory\n",
    "    calss_mode: 'categorical', 'binary', 'sparse'\n",
    "    gen: keras image generator\n",
    "    shuffle: if to shuffle data or not\n",
    "    target_size: out dimensions of the image\n",
    "    Yields\n",
    "    batch of given dimension\n",
    "    '''\n",
    "    return gen.flow_from_directory(path, class_mode=class_mode, batch_size=batch_size, \\\n",
    "                                   target_size=target_size, shuffle=shuffle)\n",
    "\n",
    "def get_steps(batches, batch_size):\n",
    "    '''Return number of times the batches to train on for keras fit_generator'''\n",
    "    steps = int(batches.samples/batch_size)\n",
    "    return (steps if batches.samples%batch_size==0 else (steps+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12954 images belonging to 3 classes.\n",
      "Found 3375 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# get train and valid batches\n",
    "train_b = get_batches(traindata_path, batch_size=batch_size)\n",
    "valid_b = get_batches(validdata_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_steps = get_steps(train_b, batch_size)\n",
    "valid_steps = get_steps(valid_b, batch_size)\n",
    "num_class = train_b.num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get class labels\n",
    "train_labels = train_b.classes\n",
    "valid_labels = valid_b.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the labels\n",
    "y_train = keras.utils.to_categorical(train_labels)\n",
    "y_valid = keras.utils.to_categorical(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12954,) (3375,) (12954, 3) (3375, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape, valid_labels.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12954 images belonging to 3 classes.\n",
      "Found 3375 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# generate batches to calculating the conv features\n",
    "# NOTE: CLASS_MODE = NONE because you we are running batches for prediction and not training\n",
    "trn_b = get_batches(traindata_path, class_mode=None, shuffle=False, batch_size=batch_size)\n",
    "val_b = get_batches(validdata_path, class_mode=None, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12954 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "gen_augdata = image.ImageDataGenerator(width_shift_range=.2, height_shift_range=.2, \n",
    "                                      shear_range=0.2, zoom_range=0.2)\n",
    "augtrain_b = get_batches(traindata_path, gen=gen_augdata, class_mode='categorical',\\\n",
    "                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models </br>\n",
    "Read more about [VGG with Keras](https://keras.io/applications/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vgg_conv():\n",
    "    '''Returns a VGG Model without last Conv Block'''\n",
    "    \n",
    "    # load the model with pretained weights from keras\n",
    "    model = VGG16(include_top=False, weights='imagenet', input_shape=(90,320,3))\n",
    "    # get the index for last conv layer\n",
    "    layers = model.layers\n",
    "    last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Conv2D][-1]\n",
    "    # get rid of last conv block\n",
    "    conv_layers = layers[:15]\n",
    "    # create a VGG model\n",
    "    conv_model = Sequential(conv_layers)\n",
    "    # add preprocess layer on top of VGG model\n",
    "    model = Sequential()\n",
    "    # Cropp the image to just keep the road, making it easy for our neural net\n",
    "    model.add(Cropping2D(cropping=((70,0), (0,0)), input_shape=(160, 320, 3)))\n",
    "    # Normalise the data to 0\n",
    "    model.add(Lambda(lambda x: (x / 255.0) - 0.5)) \n",
    "    model.add(conv_model)\n",
    "    return model\n",
    "\n",
    "def conv_layer(input_shape):\n",
    "    '''Add top for VGG based only on Conv layers'''\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=input_shape))\n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Maxpool with 1,2 to make match rows and coloumns\n",
    "    model.add(MaxPooling2D((1,2)))\n",
    "    # number of filters/channels is equal to number of labels\n",
    "    # in this case 3\n",
    "    model.add(Conv2D(3, (3,3), padding='same'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def top_layer(input_shape):\n",
    "    '''Add custom top layer to VGG'''\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ft(trn_b, train_steps, val_b, valid_steps):\n",
    "    '''Calculate the feature for VGG model'''\n",
    "    model = vgg_conv()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # get the output VGG last conv layer\n",
    "    trn_ft = model.predict_generator(trn_b, train_steps)\n",
    "    val_ft = model.predict_generator(val_b, valid_steps)\n",
    "    print(trn_ft.shape, val_ft.shape)\n",
    "    # save for further use\n",
    "    np.save(results_path + 'trn_ft.npy', trn_ft)\n",
    "    np.save(results_path + 'val_ft.npy', val_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get VGG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12954, 5, 20, 512) (3375, 5, 20, 512)\n"
     ]
    }
   ],
   "source": [
    "# calcualte the features for VGG\n",
    "get_ft(trn_b, train_steps, val_b, valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the output of VGG\n",
    "X_train = np.load(results_path + 'trn_ft.npy')\n",
    "X_valid = np.load(results_path + 'val_ft.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train top layer, and save bottlenect weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load VGG top layer to calculate bottlenect weights\n",
    "model = top_layer(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=2e-5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath1 = results_path+'vgg_bottleneck.h5'\n",
    "# define the checkpoint\n",
    "#checkpoint1 = ModelCheckpoint(filepath1, monitor='val_loss', verbose=1, save_best_only=False,\\\n",
    "#                             save_weights_only=True, mode='min', period=1)\n",
    "#callbacks1=[checkpoint1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12954 samples, validate on 3375 samples\n",
      "Epoch 1/21\n",
      "12954/12954 [==============================] - 21s - loss: 1.1966 - acc: 0.5728 - val_loss: 0.6905 - val_acc: 0.7061\n",
      "Epoch 2/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.8817 - acc: 0.6789 - val_loss: 0.6700 - val_acc: 0.7185\n",
      "Epoch 3/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.7702 - acc: 0.7138 - val_loss: 0.7315 - val_acc: 0.6960\n",
      "Epoch 4/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.6765 - acc: 0.7414 - val_loss: 0.7234 - val_acc: 0.6975\n",
      "Epoch 5/21\n",
      "12954/12954 [==============================] - 21s - loss: 0.6342 - acc: 0.7551 - val_loss: 0.7663 - val_acc: 0.6794\n",
      "Epoch 6/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.5612 - acc: 0.7869 - val_loss: 0.7321 - val_acc: 0.7108\n",
      "Epoch 7/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.5150 - acc: 0.8028 - val_loss: 0.7752 - val_acc: 0.6889\n",
      "Epoch 8/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.4509 - acc: 0.8255 - val_loss: 0.8536 - val_acc: 0.6732\n",
      "Epoch 9/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.4295 - acc: 0.8331 - val_loss: 0.8243 - val_acc: 0.6954\n",
      "Epoch 10/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.3674 - acc: 0.8585 - val_loss: 0.8515 - val_acc: 0.7067\n",
      "Epoch 11/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.3420 - acc: 0.8661 - val_loss: 0.8982 - val_acc: 0.6874\n",
      "Epoch 12/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.3025 - acc: 0.8825 - val_loss: 1.0389 - val_acc: 0.6456\n",
      "Epoch 13/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.2781 - acc: 0.8934 - val_loss: 1.0279 - val_acc: 0.6664\n",
      "Epoch 14/21\n",
      "12954/12954 [==============================] - 21s - loss: 0.2473 - acc: 0.9039 - val_loss: 1.0464 - val_acc: 0.6708\n",
      "Epoch 15/21\n",
      "12954/12954 [==============================] - 21s - loss: 0.2213 - acc: 0.9151 - val_loss: 1.1124 - val_acc: 0.6705\n",
      "Epoch 16/21\n",
      "12954/12954 [==============================] - 22s - loss: 0.2002 - acc: 0.9205 - val_loss: 1.0981 - val_acc: 0.6782\n",
      "Epoch 17/21\n",
      "12954/12954 [==============================] - 22s - loss: 0.1837 - acc: 0.9288 - val_loss: 1.1735 - val_acc: 0.6711\n",
      "Epoch 18/21\n",
      "12954/12954 [==============================] - 22s - loss: 0.1670 - acc: 0.9377 - val_loss: 1.1791 - val_acc: 0.6856\n",
      "Epoch 19/21\n",
      "12954/12954 [==============================] - 21s - loss: 0.1386 - acc: 0.9491 - val_loss: 1.1710 - val_acc: 0.6978\n",
      "Epoch 20/21\n",
      "12954/12954 [==============================] - 21s - loss: 0.1381 - acc: 0.9494 - val_loss: 1.2282 - val_acc: 0.6809\n",
      "Epoch 21/21\n",
      "12954/12954 [==============================] - 20s - loss: 0.1212 - acc: 0.9538 - val_loss: 1.2833 - val_acc: 0.6883\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=21, batch_size=32, verbose=1, validation_data=(X_valid, y_valid))\n",
    "model.save_weights(filepath1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and finetune VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_model():\n",
    "    '''Build complete VGG model'''\n",
    "    base_model = vgg_conv()\n",
    "    # freeze first 11 layers\n",
    "    for layer in base_model.layers[:11]: layer.trainable=False\n",
    "    print (base_model.output_shape[1:])\n",
    "    # create and load top layer\n",
    "    top_model = top_layer(base_model.output_shape[1:])\n",
    "    top_model.load_weights(results_path+'vgg_bottleneck.h5')\n",
    "    # join VGG with top layer\n",
    "    base_model.add(top_model)\n",
    "    \n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20, 512)\n"
     ]
    }
   ],
   "source": [
    "vgg_model = vgg_model()\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "vgg_model.compile(optimizer=adam,loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath2 = results_path+'vgg_ft.h5'\n",
    "#checkpoint2 = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1,\\\n",
    "#                              save_best_only=False, mode='min', period=1)\n",
    "#callbacks2=[checkpoint2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "203/203 [==============================] - 235s - loss: 1.4652 - val_loss: 0.8182\n",
      "Epoch 2/2\n",
      "203/203 [==============================] - 222s - loss: 1.0997 - val_loss: 0.9582\n"
     ]
    }
   ],
   "source": [
    "vgg_model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2,\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "model.save_weights(results_path+'vgg_ft1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "203/203 [==============================] - 237s - loss: 1.0078 - val_loss: 0.8358\n",
      "Epoch 2/3\n",
      "203/203 [==============================] - 219s - loss: 0.9357 - val_loss: 0.7976\n",
      "Epoch 3/3\n",
      "203/203 [==============================] - 225s - loss: 0.8878 - val_loss: 0.7986\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "vgg_model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=3,\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "model.save_weights(results_path+'vgg_ft2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "203/203 [==============================] - 232s - loss: 0.8848 - val_loss: 0.7750\n",
      "Epoch 2/2\n",
      "203/203 [==============================] - 221s - loss: 0.8259 - val_loss: 0.8495\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "vgg_model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=2,\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "model.save_weights(results_path+'vgg_ft3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "203/203 [==============================] - 232s - loss: 0.8012 - val_loss: 0.7727\n",
      "Epoch 2/5\n",
      "203/203 [==============================] - 226s - loss: 0.7566 - val_loss: 0.8443\n",
      "Epoch 3/5\n",
      "203/203 [==============================] - 221s - loss: 0.7518 - val_loss: 0.7678\n",
      "Epoch 4/5\n",
      "203/203 [==============================] - 220s - loss: 0.7401 - val_loss: 0.8264\n",
      "Epoch 5/5\n",
      "203/203 [==============================] - 225s - loss: 0.7222 - val_loss: 0.8638\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "vgg_model.fit_generator(augtrain_b, steps_per_epoch=train_steps, epochs=5,\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "model.save_weights(results_path+'vgg_ft4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "203/203 [==============================] - 175s - loss: 0.4800 - val_loss: 0.8046\n",
      "Epoch 2/2\n",
      "203/203 [==============================] - 169s - loss: 0.3701 - val_loss: 0.9397\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "vgg_model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=2,\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "model.save_weights(results_path+'vgg_ft5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "203/203 [==============================] - 177s - loss: 0.2836 - val_loss: 0.8786\n",
      "Epoch 2/3\n",
      "203/203 [==============================] - 169s - loss: 0.2364 - val_loss: 1.0083\n",
      "Epoch 3/3\n",
      "203/203 [==============================] - 169s - loss: 0.1771 - val_loss: 1.0878\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "vgg_model.fit_generator(train_b, steps_per_epoch=train_steps, epochs=3,\n",
    "                        validation_data=valid_b, validation_steps=valid_steps)\n",
    "model.save_weights(results_path+'vgg_ft6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_path = results_path+'vgg_ft.json'\n",
    "model_json = model.to_json()\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:wheelai]",
   "language": "python",
   "name": "conda-env-wheelai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
